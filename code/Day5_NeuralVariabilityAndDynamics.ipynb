{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">SWDB 2023 - Day 5 Workshop 1: What gives rise to neural variability and dynamics?</h1> \n",
    "<h3 align=\"center\">Friday, August 25th, 2023</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h2> What could models of neural variability and dynamics tell us about the brain? </h2>\n",
    "<ul>\n",
    "<li> How does stimulus influence neuronal responses and variability? \n",
    "<li> How does stimulus interact with time course of the trial?\n",
    "<li> How does neural activity depend on its past response?\n",
    "<li> How do population of neurons interact over time to shape neural dynamics?\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h2> Imports </h2>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec # Useful tool to arrange multiple plots in one figure (https://matplotlib.org/stable/api/_as_gen/matplotlib.gridspec.GridSpec.html)\n",
    "%matplotlib inline\n",
    "\n",
    "import platform\n",
    "\n",
    "import allensdk\n",
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> Load Data </h2>\n",
    "    <p> We will be working with the Allen institute's Visual Coding Neuropixel dataset from earlier in the week <p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platstring = platform.platform()\n",
    "if ('Darwin' in platstring) or ('macOS' in platstring):\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2023/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on Code Ocean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2023/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Ocean\n",
    "data_path = Path(data_root) / 'allen-brain-observatory/visual-coding-neuropixels/ecephys-cache/manifest.json'\n",
    "cache = EcephysProjectCache.from_warehouse(manifest=data_path)\n",
    "\n",
    "print(cache.get_all_session_types())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <p> The dataset consists of two subsets. We'll specifically be looking at the Brain Observatory subset </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = cache.get_session_table()\n",
    "brain_observatory_type_sessions = sessions[sessions[\"session_type\"] == \"brain_observatory_1.1\"]\n",
    "brain_observatory_type_sessions.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h3> Choose a random session to examine: 791319847 </h3>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = 791319847\n",
    "session = cache.get_session_data(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: As in Day 2, before we proceed further, we are going to patch  AllenSDK to get around a known discrepancy between the NWB files contained in the AWS bucket and what is expected by the latest version of the AllenSDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allensdk.brain_observatory.ecephys.stimulus_table import naming_utilities\n",
    "\n",
    "def passthrough_function(input):\n",
    "    return input\n",
    "\n",
    "naming_utilities.standardize_movie_numbers = passthrough_function\n",
    "session.naming_utilities = naming_utilities # this is needed to circumvent a bug in the AllenSDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> Let's explore the breakdown by brain region: </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of neurons per region\n",
    "session.structurewise_unit_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h3> Look at the effect of stimulus on neuronal variabiity </h3>\n",
    "    \n",
    "<p> Here, we'll focus on drifting gratings stimulus: </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_stimulus_table([\"drifting_gratings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "    <p><b>Exercise 1:</b>\n",
    "    <ul>\n",
    "        <li> How many distinct orientations are present?\n",
    "        <li> How long are stimuli presented for?\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distinct orientations\n",
    "# Hint: dataframe columns have a .unique() function\n",
    "\n",
    "# can you visualize the distribution of stimulus durations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> Looking at ephys data </h2>\n",
    "    <p> In order to analyze neuronal responses to the stimulus, we need to decide how we would quantify this neuronal response. Some things to consider are: \n",
    "    <ul>\n",
    "        <li> What brain areas to look at?\n",
    "        <li> Which time interval response to look at (e.g. what should be the alignment?)\n",
    "        <li> How large should time bins be to count spikes?\n",
    "        <li> When to start and stop counting for each trial?\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> For the rest of the notebook, we will:\n",
    "    <ul>\n",
    "        <li> Only look at V1 neurons\n",
    "        <li> Align spike times to the stimulus onset. \n",
    "        <li> Use a bin size of 50ms to count spikes. \n",
    "        <li> Analyze a trial interval starting 100ms before stimulus onset to 500ms after.\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Exercise 2:</b>   complete the following block of code according to the specifications above for analyzing neuronal responses\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region = \n",
    "# time_bins = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentations = session.get_stimulus_table(\"drifting_gratings\")\n",
    "# filter out \"null trials\"\n",
    "presentations = presentations[presentations.orientation != 'null']\n",
    "# select only V1 neurons\n",
    "units = session.units[session.units[\"ecephys_structure_acronym\"] == region]\n",
    "\n",
    "# use SDK function to count spikes into bins\n",
    "spikes = session.presentationwise_spike_counts(\n",
    "    stimulus_presentation_ids=presentations.index.values,  \n",
    "    bin_edges=time_bins,\n",
    "    unit_ids=units.index.values\n",
    ")\n",
    "\n",
    "# grab distinct orientations (directions)\n",
    "unique_orientations = presentations.orientation.unique()\n",
    "unique_orientations.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> Let's visualize the firing rate of each VISp neuron (averaged across stim presentations), aligned on stimulus onset\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spikes = spikes.mean(dim=\"stimulus_presentation_id\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "im = ax.pcolormesh(\n",
    "    mean_spikes[\"time_relative_to_stimulus_onset\"], \n",
    "    np.arange(mean_spikes[\"unit_id\"].size),\n",
    "    mean_spikes.T, \n",
    "    vmin=0,\n",
    "    vmax=1\n",
    ")\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set_ylabel(\"unit index\", fontsize=24)\n",
    "ax.set_xlabel(\"time relative to stimulus onset (s)\", fontsize=24)\n",
    "ax.set_title(\"peristimulus time histograms for VISp units on drifting gratings presentations\", fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> How does each neuron respond specifically to different grating orientations? </h3>\n",
    "    <ul> \n",
    "        <li> Namely, how much do differences in the presented orientations explain neural variability?\n",
    "        <li> What's the best way to visualize this?\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> One approach is to plot the spike counts of a few units as a function of time, averaged across repeats for each unique stimulus grating orientation\n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each unit, make plots of averaged responses across each orientation, aligned to stim onset\n",
    "\n",
    "# a list of interesting units predetermined by looking data/patrick_scratch/unit_frs_analysis figures\n",
    "interesting_units = [\n",
    "    951061556,\n",
    "    951061574,\n",
    "    951061715,\n",
    "    951061906,\n",
    "    951061918,\n",
    "    951061957\n",
    "]\n",
    "fig, ax = plt.subplots(len(interesting_units), 1, figsize=(10, 30))\n",
    "\n",
    "for i, unit_id in enumerate(interesting_units):\n",
    "    for orientation in unique_orientations: \n",
    "        trial_ids = presentations[presentations.orientation == orientation].index.values        \n",
    "        data = spikes.loc[trial_ids, :, unit_id]\n",
    "        mean = data.mean(dim=\"stimulus_presentation_id\")\n",
    "        ax[i].plot(data[\"time_relative_to_stimulus_onset\"], mean, label=orientation)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(\"time relative to stimulus onset\")\n",
    "    ax[i].set_ylabel(\"spike count in bin\")\n",
    "    ax[i].set_title(f\"unit {unit_id} mean spike counts by stimulus grating direction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Stop and think: </b>\n",
    "\n",
    "<p> Each line is an average, in reality it varies from trial to trial. Can you visualize the variability?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> How can we explain variability of a single neuron? </h2>\n",
    "    <ul>\n",
    "        <li> Let's focus on just a single unit for a moment, here we will look at Unit 951061556. \n",
    "        <li> What is a good model for explaining neural activity with the stimulus orientation?\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id = 951061556"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> Model 1: Stimulus-based regression model. </h2>\n",
    "    <p> One simple first model is to learn the averaged response per orientation, and assume it does not vary with time. \n",
    "    <p> Using one-hot encoding notation for the stimulus, this can be written as a linear function:\n",
    "$$ r_t = \\textbf{w}^\\text{stim} \\cdot \\textbf{s} $$\n",
    "<p> Where $r_t $ is neuron's activity, $ \\textbf{w}^\\text{stim} $ is a vector of weights that will be fit, and $\\textbf{s}$ is a one-hot encoding vector stimulus direction. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Stop and think: </b>\n",
    "\n",
    "<p> Here we have chosen to represent our stimulus as one-hot vectors of 8 different directions. We could have alternatively treated the stimulus as a single continuous variable, with our model being: $ r_t = w^\\text{stim} \\cdot s $. What are the benefits/drawbacks of each choice?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> How should we evaluate our model? </h3>\n",
    "    <p> From the lecture, you learned the loss function for fit actually specifies \"noise\" distribution. Here, we will assume for simplicity a standard Gaussian for noise and use loss metric based on mean-squared error (MSE). Note, the popular $R^2$ metric is directly related to the MSE. \n",
    "    <p> Below, we define some helper functions to \n",
    "    <ul>\n",
    "        <li> Calculate $R^2$\n",
    "        <li> Covert stimulus into one-hot encoding\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def calc_r_squared(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates r^2 metric for true vs predicted labels\n",
    "    NOTE: could also use .score() from sklearn LinearRegression here, but it's a bit unfitting since\n",
    "    we'd want to evaluate r^2 for all time points, but are sometimes training different models for each time point. \n",
    "    Args: \n",
    "        y_true, y_pred of any shape\n",
    "    \"\"\"\n",
    "    return 1 - np.sum((y_true - y_pred)**2) / np.sum(((y_true - y_true.mean()) ** 2))\n",
    "\n",
    "\n",
    "def get_one_hot_encoding(orientations):\n",
    "    \"\"\"\n",
    "    Generates a one-hot-encoding of stimulus directions\n",
    "    Args: \n",
    "        orientations: an array of orientations as degrees, of length n_trials. Assumes that\n",
    "            orientations are in 45 degree increments, with 8 unique values. \n",
    "    Returns: \n",
    "        an array of shape n_trials x 8. array has values either 0 or 1, with one 1 existing per row, \n",
    "            indicating which orientations was present for that trial. \n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((len(orientations), 8))\n",
    "    orientation_idxs = (orientations / 45).astype(int)\n",
    "    one_hot[np.arange(len(orientation_idxs)), orientation_idxs] = 1\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> To avoid overfitting, we will split our dataset into train and test sets. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42 # ensure each run has the same split\n",
    "train_idxs, test_idxs = train_test_split(presentations.index.values, test_size=0.2, random_state=random_state)\n",
    "\n",
    "orientations_train = presentations.orientation.loc[train_idxs].values\n",
    "spikes_train = spikes.loc[train_idxs, :, unit_id].values\n",
    "\n",
    "orientations_test = presentations.orientation.loc[test_idxs].values\n",
    "spikes_test = spikes.loc[test_idxs, :, unit_id].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Here, we use sklearn to fit our model. Note that since it is a linear model, and using MSE loss, we can simply use sklearn's <code>LinearRegression</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data matrices for linear regression\n",
    "num_time_steps = spikes_train.shape[1]\n",
    "X_train = get_one_hot_encoding(orientations_train).repeat(num_time_steps, axis=0)\n",
    "Y_train = spikes_train.flatten()\n",
    "X_test = get_one_hot_encoding(orientations_test).repeat(num_time_steps, axis=0)\n",
    "Y_test = spikes_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]  # one hot encoding of stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[:5]  # unit's spike counts over time bins and trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now perform linear regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "# caluculate train and test R^2\n",
    "train_score = calc_r_squared(Y_train, reg.predict(X_train))\n",
    "test_score = calc_r_squared(Y_test, reg.predict(X_test))\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "To keep track of train/test scores as we progress in our models, let's define a dict <code>model_scores</code> to hold train test scores, with keys of that dict being the model name, and values being a tuple of train/test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "model_scores[\"Model 1: \\n Stim-based\"] = (train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "And also define a helper function to help visualize and compare model scores\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_scores(model_scores):\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    model_names = [\"\" for _ in range(6)]\n",
    "    train_scores = np.zeros(6)\n",
    "    test_scores = np.zeros(6)\n",
    "    x = np.arange(6)  # the label locations\n",
    "\n",
    "    for i, k in enumerate(model_scores):\n",
    "        model_names[i] = k\n",
    "        train_scores[i] = model_scores[k][0]\n",
    "        test_scores[i] = model_scores[k][1]\n",
    "\n",
    "    ax.bar(x, train_scores, width, label=\"Train\")\n",
    "    ax.bar(x + width, test_scores, width, label=\"Test\")\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Score ($R^2$)')\n",
    "    ax.set_title('Model Train/Test scores by comparison')\n",
    "    ax.set_xticks(x + width / 2)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    ax.set_ylim(0, 0.6)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_scores(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Plot what the model predictions look like for all trials </h3>\n",
    "    <p> Now that we have trained our model for this neuron's response, we can see what it's predictions look like against the stimulus-averaged true responses\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot regression result\n",
    "unique_orientations.sort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for orientation in unique_orientations: \n",
    "    trial_ids = presentations[presentations.orientation == orientation].index.values        \n",
    "    data = spikes.loc[trial_ids, :, unit_id]\n",
    "    mean = data.mean(dim=\"stimulus_presentation_id\")\n",
    "    line, = ax.plot(data[\"time_relative_to_stimulus_onset\"], mean, label=int(orientation))\n",
    "    predictions = reg.predict(get_one_hot_encoding(np.array([orientation]).repeat(num_time_steps)))\n",
    "    ax.plot(data[\"time_relative_to_stimulus_onset\"], predictions, color=line.get_color(), linestyle=\"--\", label=f\"{int(orientation)} predicted\")\n",
    "\n",
    "lgd_bbox_to_anchor = (1., 0.5)\n",
    "lgd_loc = \"center left\"\n",
    "ax.legend(bbox_to_anchor=lgd_bbox_to_anchor, loc=lgd_loc)\n",
    "ax.set_xlabel(\"time relative to stimulus onset\")\n",
    "ax.set_ylabel(\"spike count in bin\")\n",
    "ax.set_title(f\"unit {unit_id} mean spike counts by stimulus grating direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "By design, the predicted response to each orientation can not vary with time, therefore fails to capture the unit's variation over time. How could we improve our model?\n",
    "</div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "    <p><b>Exercise 3:</b>\n",
    "    <ul>\n",
    "        <li> Visualize the predicted response by orientation. In other words, plot out the orientation tuning curve of this neuron\n",
    "        <li> Is the neuron orientation or direction selective?\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise code here\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Observation </h3>\n",
    "    <p> unit 951061556 is an orientation-selective unit instead of a direction-selective one, and most sensitive to horizontal gratings (equally responsive to 0, 180, 360-degree gratings)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> Model 2: Stimulus based regression with time-varying weights </h2>\n",
    "    <p> The previous model made the simpifying assumption that stimulus orientation's contribution to neuronal response is constant across the time course of a trial, hence was only able to predict constant responses over time. What if we loosened this assumption a bit, and fit a separate regression per-time-bin? Our model now becomes: \n",
    "$$ r_t = \\textbf{w}^{stim}_t \\cdot \\textbf{s} $$\n",
    "    <p> Where: \n",
    "    <ul>\n",
    "        <li> $ \\textbf{w}^{stim}_t $ is still a vector of learned weights, but varies by time\n",
    "        </li> $ \\textbf{s} $ is still one-hot encoding vector stimulus direction. This has no time component as the stimulus direction does not change within a trial. \n",
    "    </ul>\n",
    "    <p> Effectively we are training separate regression models per time bin\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Stop and think:</b>\n",
    "What are the drawbacks of this model? What are some interesting insights this model <b>could miss</b>?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Train models </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_results = []\n",
    "Y_train_preds = []\n",
    "Y_test_preds = []\n",
    "\n",
    "# per time bin, train a separate model\n",
    "for i in range(spikes_train.shape[1]):\n",
    "    reg = LinearRegression()\n",
    "    X_train = get_one_hot_encoding(orientations_train)\n",
    "    Y_train = spikes_train[:, i]\n",
    "\n",
    "    X_test = get_one_hot_encoding(orientations_test)\n",
    "\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    lin_reg_results.append(reg)    \n",
    "    Y_train_preds.append(reg.predict(X_train))\n",
    "    Y_test_preds.append(reg.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Evaluate train and test scores </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = calc_r_squared(spikes_train, np.vstack(Y_train_preds).T)\n",
    "test_score = calc_r_squared(spikes_test, np.vstack(Y_test_preds).T)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Now, we can compare our model performance against the previous Model 1 \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores[\"Model 2: \\n Stim with \\n time varying\"] = (train_score, test_score)\n",
    "plot_model_scores(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Visualize model predictions </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot regression result\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for orientation in unique_orientations: \n",
    "    trial_ids = presentations[presentations.orientation == orientation].index.values        \n",
    "    data = spikes.loc[trial_ids, :, unit_id]\n",
    "    mean = data.mean(dim=\"stimulus_presentation_id\")\n",
    "    line, = ax.plot(data[\"time_relative_to_stimulus_onset\"], mean, label=int(orientation))\n",
    "    predictions = [reg.predict(get_one_hot_encoding(np.array([orientation]))) for reg in lin_reg_results]\n",
    "    ax.plot(data[\"time_relative_to_stimulus_onset\"], predictions, color=line.get_color(), linestyle=\"--\", label=f\"{int(orientation)} predicted\")\n",
    "lgd_bbox_to_anchor = (1., 0.5)\n",
    "lgd_loc = \"center left\"\n",
    "ax.legend(bbox_to_anchor=lgd_bbox_to_anchor, loc=lgd_loc)\n",
    "ax.set_xlabel(\"time relative to stimulus onset\")\n",
    "ax.set_ylabel(\"spike count in bin\")\n",
    "ax.set_title(f\"unit {unit_id} mean spike counts by stimulus grating direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> What about the rest of the variability? </h2>\n",
    "    <p> So far, our model has gotten fairly good at matching the stimulus-averaged responses of this neuron, though looking at the $ R^2 $ score, this still doesn't explain all of the neural variability. How variable is neuron's activity within the same stimulus condition?\n",
    "    <p> To answer this question, we can plot the per-trial activity of our neuron within the same stimulus condition, plotting the mean, standard deviation, and a few example trial activities.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_example_trials_to_plot = 5\n",
    "fig, axs = plt.subplots(4,2, figsize=(8,12), dpi=200)\n",
    "\n",
    "for orientation_id, orientation in enumerate(unique_orientations):\n",
    "    row = int(orientation_id% 4)\n",
    "    col = int(orientation_id/ 4)\n",
    "    ax = axs[row, col]\n",
    "    \n",
    "    trial_ids = presentations[presentations.orientation == orientation].index.values \n",
    "    data = spikes.loc[trial_ids, :, unit_id]\n",
    "    mean = data.mean(dim=\"stimulus_presentation_id\")\n",
    "    mean_line, = ax.plot(data[\"time_relative_to_stimulus_onset\"], mean, label=\"mean and std\", lw=3)\n",
    "    std = data.std(dim=\"stimulus_presentation_id\")\n",
    "    std_line = ax.fill_between(data[\"time_relative_to_stimulus_onset\"], mean - std, mean + std, color=mean_line.get_color(), alpha=0.5)\n",
    "\n",
    "    for i in range(num_example_trials_to_plot):\n",
    "          ax.plot(data[\"time_relative_to_stimulus_onset\"], data[i, :], label=f\"example trial {i}\", alpha=0.8)\n",
    "    ax.set_xlabel(\"time relative to stimulus onset\")\n",
    "    ax.set_ylabel(\"spike count in bin\")\n",
    "    ax.set_title(f\"{int(orientation)} degree\")\n",
    "\n",
    "plt.legend()    \n",
    "fig.suptitle(f\"unit {unit_id} neural variability\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <p><b>Observation: </b></p>\n",
    "    <ul>\n",
    "        <li> There is substantial variability within stimulus condition across trials\n",
    "        <li> Within each trial, the neural response trajectory is **somewhat** smooth $\\to$ there are some temporal correlations\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h1> Capturing the temporal relationship in neural responses </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> Model 3: stimulus + 1 time bin back regression </h2>\n",
    "    <p> One idea for trying to explain stimulus-conditioned trial-by-trial variability is to assume that a neuron's activity at a certain timepoint depends on its activity at the previous timepoint. This would mean introducing an *auto-regressive* component to our model: \n",
    "    <p> $$ r_t = \\textbf{w}^\\text{stim}_t \\cdot \\textbf{s} + w^\\text{auto}_{t} r_{t-1} $$\n",
    "    <p> Where:\n",
    "    <ul> \n",
    "        <li> $ w^\\text{auto}_t $ is the learned weight capturing how previous time bin $t - 1$ activity influences the target time bin $t$ activity. Note: here we're still learning a different weight per time bin. \n",
    "        <li> $ r_{t-1} $ is our neuron's activity at $t - 1$\n",
    "    </ul>\n",
    "<p> With this model, we ask: per timestep, is variation in activity explained by stim orientation as well as previous activity?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <p> First, we can define a helper function that will help generate inputs for our model, from both the stimulus orientation and the unit's past spiking data:\n",
    "    <p> Note: we're defining a more generic version here, with a <code>lag</code> parameter. This will be helpful in later models, where we introduce the concept of lag. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_auto_reg(spikes, orientations, t, lag=1):\n",
    "    \"\"\"\n",
    "    Generates a input data matrix for the lag time bin auto regressive model\n",
    "    Args:\n",
    "        spikes: data array of num_trials x time steps\n",
    "        orientations: data array of num_trials\n",
    "        t: index of time bin\n",
    "    \"\"\"\n",
    "    orientations_one_hot = get_one_hot_encoding(orientations)\n",
    "    spikes_design_matrix = spikes[:, t-lag:t]\n",
    "    \n",
    "    return np.concatenate([orientations_one_hot, spikes_design_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Let's test out this function and see what it returns\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5\n",
    "X = get_X_auto_reg(spikes_train, orientations_train, t)\n",
    "print(X[:5, :8])\n",
    "print(X[:5, -1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Next we'll train our model. Here, we're still training our model per time bin\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_reg_results = []\n",
    "Y_train_preds = []\n",
    "Y_test_preds = []\n",
    "for i in range(spikes_train.shape[1]):\n",
    "    if i >= 1:\n",
    "        reg = LinearRegression()\n",
    "        X_train = get_X_auto_reg(spikes_train, orientations_train, i)\n",
    "        X_test = get_X_auto_reg(spikes_test, orientations_test, i)\n",
    "\n",
    "        Y_train = spikes_train[:, i]\n",
    "        reg.fit(X_train, Y_train)\n",
    "        auto_reg_results.append(reg)\n",
    "\n",
    "        Y_train_preds.append(reg.predict(X_train))\n",
    "        Y_test_preds.append(reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = calc_r_squared(spikes_train[:, 1:], np.vstack(Y_train_preds).T)\n",
    "test_score = calc_r_squared(spikes_test[:, 1:], np.vstack(Y_test_preds).T)\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores[\"Model 3: \\n Time varying stim and \\n autoregression (lag = 1)\"] = (train_score, test_score)\n",
    "plot_model_scores(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Observation </h3>\n",
    "    <p> Autoregressive models with 1 time-bin back history add slight improvements to the $R^2$ score (from 0.49 to 0.52 on the test set.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Visualize Autoregressive model results </h3>\n",
    "    <p> To see how well our model is capturing stimulus-conditioned variability, we can take some example trials from a specific stimulus direction (0 degrees) and plot the true trial activity (in solid lines) against the predicted trial activity (in dotted lines). \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot regression result\n",
    "def plot_example_trial_results(spikes, models, orientation, n_examples, get_X_func, lag=1, all_spikes_for_reg=False):\n",
    "    \"\"\"\n",
    "    Plot the first n example trial activities of a specific orientation \n",
    "    against predicted activities from a model\n",
    "    Args: \n",
    "        spikes: data matrix containing trial spike activity, trials x times x units\n",
    "        models: \n",
    "        orientation: orientation to sub-select for\n",
    "        n_examples: number of examples to plot\n",
    "        get_X_func: function to convert spikes/orientations to regression inputs\n",
    "        lag: how much lag to include for autoregression\n",
    "        all_spikes_for_reg: whether or not to pass all units spikes into \n",
    "    \"\"\"\n",
    "    trial_ids = presentations[presentations.orientation == orientation].index.values    \n",
    "    trial_spikes = spikes.loc[trial_ids, :, :]\n",
    "    ex_spikes = trial_spikes[:n_examples, : , :]\n",
    "    ex_unit_data = ex_spikes.loc[:, :, unit_id]\n",
    "    repeat_orientations = np.repeat(orientation, n_examples)\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    for t in range(lag, ex_unit_data.shape[1]):\n",
    "        model = models[0] if len(models) == 1 else models[t-lag]\n",
    "        if all_spikes_for_reg:\n",
    "            X = get_X_func(ex_spikes, repeat_orientations, t, lag)\n",
    "        else:\n",
    "            X = get_X_func(ex_unit_data, repeat_orientations, t, lag)\n",
    "        Y = model.predict(X)\n",
    "        predictions.append(Y)\n",
    "    predictions = np.vstack(predictions).T\n",
    "    time_bins = data[\"time_relative_to_stimulus_onset\"]\n",
    "    fig, ax = plt.subplots(n_examples, 1, figsize=(4, n_examples * 2.5))\n",
    "    for i in range(ex_unit_data.shape[0]):\n",
    "        line, = ax[i].plot(time_bins, ex_unit_data[i, :], label=\"True trial Activity\")  \n",
    "        ax[i].plot(time_bins[lag:], predictions[i, :], label=\"Predicted\", color=line.get_color(), linestyle=\"--\")\n",
    "        ax[i].set_xlabel(\"time relative to stimulus onset\")\n",
    "        ax[i].set_ylabel(\"predicted spike count in bin\")\n",
    "    plt.legend()    \n",
    "    fig.suptitle(f\"unit {unit_id} example traces for orientation {orientation} degrees\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "# plot regression result\n",
    "def plot_averaged_auto_regressive_predictions(spikes, models, unique_orientations, get_X_func, lag=1, all_spikes_for_reg=False):\n",
    "    \"\"\"\n",
    "    Helper function to plot predictions from auto regressive models \n",
    "    averaged by stimulus, against true neural activity also averaged by stimulus\n",
    "    Args: \n",
    "        spikes: spike data, of trials x time x unit\n",
    "        \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    for orientation in unique_orientations: \n",
    "        trial_ids = presentations[presentations.orientation == orientation].index.values        \n",
    "        data = spikes.loc[trial_ids, :, unit_id]\n",
    "\n",
    "        trial_spikes = spikes.loc[trial_ids, :, :]\n",
    "        unit_data = trial_spikes.loc[:, :, unit_id]\n",
    "        \n",
    "        \n",
    "        time_bins= unit_data[\"time_relative_to_stimulus_onset\"]\n",
    "        num_tim_bins = len(time_bins)\n",
    "\n",
    "        mean = unit_data.mean(dim=\"stimulus_presentation_id\")\n",
    "        line, = ax.plot(time_bins, mean, label=int(orientation))\n",
    "        pred_mean_responses = np.empty(num_tim_bins - lag)\n",
    "        orientations = np.repeat(orientation, len(trial_ids))\n",
    "        for i in range(num_tim_bins - lag):\n",
    "            model = models[0] if len(models) == 1 else models[i]\n",
    "            time_bin_idx = i + lag\n",
    "            if all_spikes_for_reg:\n",
    "                X = get_X_func(trial_spikes, orientations, time_bin_idx, lag)\n",
    "            else:\n",
    "                X = get_X_func(unit_data, orientations, time_bin_idx, lag)\n",
    "            Y = model.predict(X)\n",
    "            pred_mean_responses[i] = np.mean(Y)    \n",
    "        ax.plot(time_bins[lag:], pred_mean_responses, color=line.get_color(), linestyle=\"--\", label=f\"{int(orientation)} predicted\")\n",
    "    lgd_bbox_to_anchor = (1., 0.5)\n",
    "    lgd_loc = \"center left\"\n",
    "    ax.legend(bbox_to_anchor=lgd_bbox_to_anchor, loc=lgd_loc)\n",
    "    ax.set_xlabel(\"time relative to stimulus onset\")\n",
    "    ax.set_ylabel(\"spike count in bin\")\n",
    "    ax.set_title(f\"unit {unit_id} mean spike counts by stimulus grating direction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_trial_results(\n",
    "    spikes=spikes, \n",
    "    models=auto_reg_results, \n",
    "    orientation=0, \n",
    "    n_examples=4,\n",
    "    get_X_func=get_X_auto_reg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "    <p><b> Exercise: 4</b></p>\n",
    "    <p> Redo the plots above for different orientations, use the defined helper function `plot_example_trial_results()`, and refer to how it is used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "We can also look at how well our model predicts trial-averaged, stimulus-conditioned responses over time\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_averaged_auto_regressive_predictions(\n",
    "    spikes=spikes,\n",
    "    models=auto_reg_results,\n",
    "    unique_orientations=unique_orientations,\n",
    "    get_X_func=get_X_auto_reg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> Model 4: Same autoregressive model, longer temporal dependency </h2>\n",
    "    <p> Previously, our model only depended on the unit's own activity one time step back, but what if we added more time steps? Our model becomes:\n",
    "    <p> $$ r_t = \\textbf{w}^\\text{stim}_t \\cdot \\textbf{s} + \\sum_{l=1}^L w^\\text{auto}_{t,l} r_{t-l} $$\n",
    "    <p> Where: \n",
    "    <ul>\n",
    "        <li> $L$ is the maximum number of time steps back to consider, or the lag\n",
    "        <li> $ w^\\text{auto}_{t,l} $ is the weight learned for how the neural activity $l$ time steps back (or $ r_{t-l} $) influences current neural activity at time $ t $\n",
    "    </ul>\n",
    "    <p> How does the $R^2$ vary with choice of lag L we choose?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "    <p><b>Stop and think:</b></p>\n",
    "    <p> What could be some insights from this model? What are some limitations?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> In the cell below, we'll loop through different lag (L) values, and fit a series of linear regressions per time bin for each lag. \n",
    "<p> We'll use the same <code>get_X_auto_reg()</code> function defined previously to generate our inputs, this time passing in a <code>lag</code> parameter\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "auto_reg_results = {}\n",
    "\n",
    "for n_lag in range(0, 8):\n",
    "    auto_reg_fits = []\n",
    "    Y_train_preds = []\n",
    "    Y_test_preds = []\n",
    "    \n",
    "    for time_idx in range(spikes_train.shape[1]):\n",
    "        if time_idx >= n_lag:\n",
    "            reg = LinearRegression()\n",
    "            # grab \n",
    "            X_train = get_X_auto_reg(spikes_train, orientations_train, time_idx, lag=n_lag)\n",
    "            X_test = get_X_auto_reg(spikes_test, orientations_test, time_idx, lag=n_lag)\n",
    "\n",
    "            Y_train = spikes_train[:, time_idx]\n",
    "            reg.fit(X_train, Y_train)\n",
    "            auto_reg_fits.append(reg)\n",
    "\n",
    "            Y_train_preds.append(reg.predict(X_train))\n",
    "            Y_test_preds.append(reg.predict(X_test))\n",
    "            \n",
    "    train_score = calc_r_squared(spikes_train[:, n_lag:], np.vstack(Y_train_preds).T)\n",
    "    test_score = calc_r_squared(spikes_test[:, n_lag:], np.vstack(Y_test_preds).T)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    auto_reg_results[n_lag] = auto_reg_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(0, 8), train_scores, label=\"Train\")\n",
    "ax.plot(range(0, 8), test_scores, label=\"Test\")\n",
    "ax.set_xlabel(\"Time Lag\")\n",
    "ax.set_ylabel(\"Score ($R^2$)\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Let's plot the model with lag = 6 performance alongside all the previous models\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores[\"Model 4: \\n Time varying stim and \\n autoregression (lag = 6)\"] = (train_scores[6], test_scores[6])\n",
    "plot_model_scores(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Visualize example predictions from lag = 6 model </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_trial_results(\n",
    "    spikes=spikes, \n",
    "    models=auto_reg_results[6], \n",
    "    orientation=0, \n",
    "    n_examples=4,\n",
    "    get_X_func=get_X_auto_reg,\n",
    "    lag=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <p><b>Stop and think</b></p>\n",
    "    <p> Can you spot an issue with the current evaluation of models with time lag, especially with higher lags? How would you fix it?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_averaged_auto_regressive_predictions(\n",
    "    spikes=spikes,\n",
    "    models=auto_reg_results[6],\n",
    "    unique_orientations=unique_orientations,\n",
    "    get_X_func=get_X_auto_reg,\n",
    "    lag=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Observation </h3>\n",
    "    <p> adding more lags to the autoregressive models may increases the $R^2$ score. In this unit, with n_lag=6 we have the best model with $R^2$ score up to 0.55 in the test set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h3> Gaining insights from the model: how do auto-regressive weights change over the course of a trial? </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use time_lag=1 model as an exmaple\n",
    "lag_model = 1\n",
    "reg_coeff = []\n",
    "for i, reg in enumerate(auto_reg_results[lag_model]):\n",
    "    reg_coeff.append(reg.coef_[8:])\n",
    "reg_coeff = np.array(reg_coeff)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), dpi=150)\n",
    "\n",
    "ax = axs[0]\n",
    "for j in range(reg_coeff.shape[1]):\n",
    "    ax.plot(np.arange(lag_model, lag_model+reg_coeff.shape[0]), reg_coeff[:, j], \n",
    "            label=f'lag {lag_model-j}')\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_xlabel(\"target bin index\")\n",
    "ax.set_ylabel(\"weight\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "im = ax.imshow(reg_coeff.T, cmap='RdBu', vmin=-1, vmax=1)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(reg_coeff.shape[0]))\n",
    "ax.set_xticklabels(np.arange(lag_model, lag_model+reg_coeff.shape[0]))\n",
    "ax.set_yticks(np.arange(reg_coeff.shape[1]))\n",
    "ax.set_yticklabels(np.arange(reg_coeff.shape[1], 0, -1))\n",
    "ax.set_xlabel(\"target bin index\")\n",
    "\n",
    "fig.suptitle(\"Autoregression weights across time bins in a trial\")\n",
    "fig.colorbar(im, ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use time_lag=6 model as an exmaple\n",
    "\n",
    "lag_model = 6\n",
    "reg_coeff = []\n",
    "for i, reg in enumerate(auto_reg_results[lag_model]):\n",
    "    reg_coeff.append(reg.coef_[8:])\n",
    "reg_coeff = np.array(reg_coeff)\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), dpi=150)\n",
    "\n",
    "ax = axs[0]\n",
    "for j in range(reg_coeff.shape[1]):\n",
    "    ax.plot(np.arange(lag_model, lag_model+reg_coeff.shape[0]), reg_coeff[:, j], \n",
    "            label=f'lag {lag_model-j}')\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_xlabel(\"target bin index\")\n",
    "ax.set_ylabel(\"weight\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[1]\n",
    "im = ax.imshow(reg_coeff.T, cmap='RdBu', vmin=-1, vmax=1)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(reg_coeff.shape[0]))\n",
    "ax.set_xticklabels(np.arange(lag_model, lag_model+reg_coeff.shape[0]))\n",
    "ax.set_yticks(np.arange(reg_coeff.shape[1]))\n",
    "ax.set_yticklabels(np.arange(reg_coeff.shape[1], 0, -1))\n",
    "ax.set_xlabel(\"target bin index\")\n",
    "ax.set_ylabel(\"lag\")\n",
    "\n",
    "fig.suptitle(\"Autoregression weight across time bins in a trial\")\n",
    "fig.colorbar(im, ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3>Observation </h3>\n",
    "    <p> The weights for autoregression seem reaonsably consistent across different target time bins. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> Model 5: Auto regressive model with uniform weights </h2>\n",
    "    <p> In many of our previous models, we fit different models to each time point separately. While this fit the data well, it does limit the potential insights we can gain from the model, as it does not offer a uniform solution across time. \n",
    "    <p> We had previously justified this decision by deciding that without a time component, stimulus alone was fairly poor at predicting neural activity. However is this still true with the introduction of the autoregressive term?\n",
    "<p> A model where we don't consider each time point separately can be written as:\n",
    "<p> $$ r_t = \\textbf{w}^\\text{stim} \\cdot \\textbf{s} + \\sum_{l=1}^{L} w^\\text{auto}_{l}  r_{t-l} $$\n",
    "<p> <b> Note: </b> $ \\textbf{w}^\\text{stim} $ and $ w^\\text{auto}_{l} $ are now fixed across time bins $ t $\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Stop and think:</b>  \n",
    "<p> What new insights could be gained from this model, assuming it fits the data just as well as previous models?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> stimulus + 1 time bin back (L=1) </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_auto_reg(spikes, orientations, i, lag=1):\n",
    "    \"\"\"\n",
    "    Generates a input data matrix for the lag time bin auto regressive model\n",
    "    Args:\n",
    "        spikes: data array of num_trials x time steps\n",
    "        orientations: data array of num_trials\n",
    "        i: index of time bin\n",
    "    \"\"\"\n",
    "    orientations_one_hot = get_one_hot_encoding(orientations)\n",
    "    spikes_design_matrix = spikes[:, i-lag:i]\n",
    "    \n",
    "    return np.concatenate([orientations_one_hot, spikes_design_matrix], axis=1)\n",
    "\n",
    "X_train, Y_train = [], []\n",
    "X_test, Y_test = [], []\n",
    "\n",
    "Y_train_preds = []\n",
    "Y_test_preds = []\n",
    "\n",
    "for i in range(spikes_train.shape[1]):\n",
    "    # iterate across time bins\n",
    "    if i >= 1:\n",
    "        X_train_i = get_X_auto_reg(spikes_train, orientations_train, i)\n",
    "        X_test_i = get_X_auto_reg(spikes_test, orientations_test, i)\n",
    "        Y_train_i = spikes_train[:, i]\n",
    "        Y_test_i = spikes_test[:, i]\n",
    "\n",
    "        X_train.append(X_train_i)\n",
    "        X_test.append(X_test_i)\n",
    "        Y_train.append(Y_train_i)\n",
    "        Y_test.append(Y_test_i)\n",
    "\n",
    "X_train = np.array(X_train).reshape(-1, 9)\n",
    "X_test = np.array(X_test).reshape(-1, 9)\n",
    "Y_train = np.concatenate(Y_train)\n",
    "Y_test = np.concatenate(Y_test)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_preds.append(reg.predict(X_train))\n",
    "Y_test_preds.append(reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = calc_r_squared(Y_train, np.vstack(Y_train_preds))\n",
    "test_score = calc_r_squared(Y_test, np.vstack(Y_test_preds))\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores[\"Model 5: \\n Fixed stim and \\n autoregression (lag = 1)\"] = (train_score, test_score)\n",
    "plot_model_scores(model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h3> Observation </h3>\n",
    "<p> This is performing better than Model 1, the fixed stimulus model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Visualize against stim-averaged activity: </h3>\n",
    "    <p> Is the new autoregressive model able to capture stimulus-averaged activity?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_averaged_auto_regressive_predictions(\n",
    "    spikes=spikes,\n",
    "    models=[reg],\n",
    "    unique_orientations=unique_orientations,\n",
    "    get_X_func=get_X_auto_reg,\n",
    "    lag=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_trial_results(\n",
    "    spikes=spikes, \n",
    "    models=[reg], \n",
    "    orientation=0, \n",
    "    n_examples=4,\n",
    "    get_X_func=get_X_auto_reg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Observation: </h3>\n",
    "    <p> the model shared weights performs similarly well with time-varying weight models\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Using longer history (varying lag) </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_reg_fixed_weights_fits = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for T in range(1, 10):\n",
    "    print(f'fitting model of lag {T}')\n",
    "    X_train, Y_train = [], []\n",
    "    X_test, Y_test = [], []\n",
    "\n",
    "    auto_reg_fits = []\n",
    "    Y_train_preds = []\n",
    "    Y_test_preds = []\n",
    "\n",
    "    for i in range(spikes_train.shape[1]):\n",
    "        if i >= T:\n",
    "            X_train_i = get_X_auto_reg(spikes_train, orientations_train, i, T)\n",
    "            X_test_i = get_X_auto_reg(spikes_test, orientations_test, i, T)\n",
    "            Y_train_i = spikes_train[:, i]\n",
    "            Y_test_i = spikes_test[:, i]\n",
    "\n",
    "            X_train.append(X_train_i)\n",
    "            X_test.append(X_test_i)\n",
    "            Y_train.append(Y_train_i)\n",
    "            Y_test.append(Y_test_i)\n",
    "\n",
    "    X_train = np.array(X_train).reshape(-1, T+8)\n",
    "    X_test = np.array(X_test).reshape(-1, T+8)\n",
    "    Y_train = np.array(Y_train).reshape(-1, 1)\n",
    "    Y_test = np.array(Y_test).reshape(-1, 1)\n",
    "\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train, Y_train)\n",
    "    auto_reg_fixed_weights_fits.append(reg)\n",
    "\n",
    "    Y_train_preds.append(reg.predict(X_train))\n",
    "    Y_test_preds.append(reg.predict(X_test))\n",
    "\n",
    "    # evaluate bin index [T: )\n",
    "    train_score = calc_r_squared(Y_train, np.vstack(Y_train_preds))\n",
    "    test_score = calc_r_squared(Y_test, np.vstack(Y_test_preds))\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, 10), train_scores, label=\"Train\")\n",
    "ax.plot(range(1, 10), test_scores, label=\"Test\")\n",
    "ax.set_xlabel(\"Time Lag\")\n",
    "ax.set_ylabel(\"Score ($R^2$)\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Visualize stimulus averaged conditions with lag = 3 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_averaged_auto_regressive_predictions(\n",
    "    spikes=spikes,\n",
    "    models=[auto_reg_fixed_weights_fits[2]],\n",
    "    unique_orientations=unique_orientations,\n",
    "    get_X_func=get_X_auto_reg,\n",
    "    lag=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> check regression coefficients </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lag = 1\n",
    "reg_model = auto_reg_fixed_weights_fits[model_lag-1]\n",
    "print(f'lag-{model_lag} model coefficients')\n",
    "print(f' {reg_model.coef_[0, 8:]}')\n",
    "\n",
    "model_lag = 5\n",
    "reg_model = auto_reg_fixed_weights_fits[model_lag-1]\n",
    "auto_coefs = reg_model.coef_[0, 8:]\n",
    "print(f'lag-{model_lag} model coefficients')\n",
    "print(auto_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Let's plot the lag-5 model coefficients. Here, the higher the lag, the further back we look, so we'll plot this in the opposite direction.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(auto_coefs, label=\"autoregressive weights\")\n",
    "ax.set_xticks(np.arange(5))\n",
    "ax.set_xticklabels(np.arange(5)[::-1])\n",
    "ax.set_xlabel(\"lags\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h3> Observation </h3>\n",
    "<p> In general, it seems that weights corresponding to earlier activities are lower, indicating that more recent activity has a larger effect than less recent. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h1> How can we examine the interaction among units and how it might contribute to neural dynamics? </h1>\n",
    "    <p> Thus far, each neuron has been modeled entirely separately from each other. However, it's known that neurons are highly inter-connected and bound to influence each other. Given this, by modeling the interactions between neurons, we might be able to: \n",
    "    <ol>\n",
    "        <li> Improve predictive performance of our model\n",
    "        <li> Learn and predict interactions between units\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2>Model 6: model incorporating neuron-to-neuron interaction</h2>\n",
    "    <p> In our new model formulation, to predict a neuron's activity, we'll use the neuron's previous activity as well as other neurons' previous activity, written as: \n",
    "    <p> $$ r_{i,t} = \\textbf{w}^\\text{stim} \\cdot \\textbf{s} + \\sum_{j=1}^{N_{units}} w^\\text{inter}_{ij} r_{j, t-1}$$\n",
    "    <p> Here:\n",
    "    <ul>\n",
    "       <li> $ r_{i,t} $ indicate's neuron $i$'s activity at time $t$\n",
    "       <li> $ w^\\text{inter}_{ij} $ is the weight learned for how past activity of neuron $j$, or $r_{j, t-1}$ should influence current activity of neuron $i$, or $r_{i,t}$. \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Stop and think:</b>  \n",
    "<p> What are the limitations of this model? What do insights do we gain?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "To start implementing, let's first define some neurons of interest, and re-split our data into train/test sets\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a subset of V1 neurons for prediction\n",
    "interesting_units = [\n",
    "    951061556,\n",
    "    951061574,\n",
    "    951061715,\n",
    "    951061906,\n",
    "    951061918,\n",
    "    951061957\n",
    "]\n",
    "target_unit_id = 951061556\n",
    "\n",
    "presentations.index.values\n",
    "random_state = 42 # ensure each run has the same split\n",
    "train_idxs, test_idxs = train_test_split(presentations.index.values, test_size=0.2, random_state=random_state)\n",
    "spikes_interesting = spikes.loc[:, :, interesting_units]\n",
    "orientations_train = presentations.orientation.loc[train_idxs].values\n",
    "spikes_train = spikes_interesting.loc[train_idxs, :, :].values\n",
    "\n",
    "orientations_test = presentations.orientation.loc[test_idxs].values\n",
    "spikes_test = spikes_interesting.loc[test_idxs, :, :].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> stimulus + 1 time bin back </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_auto_reg_population(spikes, orientations, i, lag=1):\n",
    "    \"\"\"\n",
    "    Generates a input data matrix for the 1 time bin auto regressive model\n",
    "    Args:\n",
    "        spikes: data array of num_trials x time steps x num_units\n",
    "        orientations: data array of num_trials\n",
    "        i: index of time bin\n",
    "    \"\"\"\n",
    "    orientations_one_hot = get_one_hot_encoding(orientations) \n",
    "    return np.hstack([orientations_one_hot, spikes[:, i-1, :]])\n",
    "\n",
    "X_train, Y_train = [], []\n",
    "X_test, Y_test = [], []\n",
    "\n",
    "Y_train_preds = []\n",
    "Y_test_preds = []\n",
    "\n",
    "for i in range(spikes_train.shape[1]):\n",
    "    if i >= 1:\n",
    "        X_train_i = get_X_auto_reg_population(spikes_train, orientations_train, i)\n",
    "        X_test_i = get_X_auto_reg_population(spikes_test, orientations_test, i)\n",
    "        Y_train_i = spikes_train[:, i, 0]\n",
    "        Y_test_i = spikes_test[:, i, 0]\n",
    "\n",
    "        X_train.append(X_train_i)\n",
    "        X_test.append(X_test_i)\n",
    "        Y_train.append(Y_train_i)\n",
    "        Y_test.append(Y_test_i)\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "X_test = np.vstack(X_test)\n",
    "Y_train = np.concatenate(Y_train)\n",
    "Y_test = np.concatenate(Y_test)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_preds.append(reg.predict(X_train))\n",
    "Y_test_preds.append(reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = calc_r_squared(Y_train, np.vstack(Y_train_preds))\n",
    "test_score = calc_r_squared(Y_test, np.vstack(Y_test_preds))\n",
    "\n",
    "print(f\"Train Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores[\"Model 6: \\n Fixed stim and \\n population interactions\"] = (train_score, test_score)\n",
    "plot_model_scores(model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_trial_results(\n",
    "    spikes=spikes_interesting,\n",
    "    models=[reg],\n",
    "    orientation=0,\n",
    "    n_examples=4,\n",
    "    get_X_func=get_X_auto_reg_population,\n",
    "    lag=1,\n",
    "    all_spikes_for_reg=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_averaged_auto_regressive_predictions(\n",
    "    spikes=spikes_interesting,\n",
    "    models=[reg],\n",
    "    unique_orientations=unique_orientations,\n",
    "    get_X_func=get_X_auto_reg_population,\n",
    "    lag=1,\n",
    "    all_spikes_for_reg=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check influence from other neurons, $ \\mathbf{w}^\\text{inter} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'target_unit: {target_unit_id}')\n",
    "for i, unit in enumerate(interesting_units):\n",
    "    print(f' unit {unit} coeff: {reg.coef_[i+8]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Observation: </h3>\n",
    "    <ul>\n",
    "        <li> dependence on self previous activity is the strongest (weight 0.335).\n",
    "        <li> weights are all positive\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h2> Population prediction </h2>\n",
    "    <p> We can actually run this prediction for multiple neurons at once. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Exercise 6:</b></p>\n",
    "<p> Our previous model equation: \n",
    "$$ r_{i,t} = \\textbf{w}^\\text{stim} \\cdot \\textbf{s} + \\sum_{j=1}^{N_{units}} w^\\text{inter}_{ij} r_{j, t-1}$$\n",
    "can also be written in vector form, where $ r_{i,t} $ becomes a vector $ \\mathbf{r}_t $ representing population activity. In that form, what do the corresponding weight terms $\\textbf{w}^\\text{stim}$ and $w^\\text{inter}_{ij}$ look like? Write out the full model equation. Does it remind you of anything introduced in lecture?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "Train and test the population model: \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_auto_reg_population(spikes, orientations, i):\n",
    "    \"\"\"\n",
    "    Generates a input data matrix for the 1 time bin auto regressive model\n",
    "    Args:\n",
    "        spikes: data array of num_trials x time steps x num_units\n",
    "        orientations: data array of num_trials\n",
    "        i: index of time bin\n",
    "    \"\"\"\n",
    "    orientations_one_hot = get_one_hot_encoding(orientations)\n",
    "    return np.concatenate([orientations_one_hot, spikes[:, i-1, :]], axis=1)\n",
    "\n",
    "\n",
    "X_train_dict, Y_train_dict = {}, {}\n",
    "X_test_dict, Y_test_dict = {}, {}\n",
    "reg_dict = {}\n",
    "Y_train_preds_dict = {}\n",
    "Y_test_preds_dict = {}\n",
    "\n",
    "\n",
    "for unit_i, unit in enumerate(interesting_units):\n",
    "    X_train, Y_train = [], []\n",
    "    X_test, Y_test = [], []\n",
    "\n",
    "    Y_train_preds = []\n",
    "    Y_test_preds = []\n",
    "\n",
    "    for i in range(spikes_train.shape[1]):\n",
    "        if i >= 1:\n",
    "            X_train_i = get_X_auto_reg_population(spikes_train, orientations_train, i)\n",
    "            X_test_i = get_X_auto_reg_population(spikes_test, orientations_test, i)\n",
    "            Y_train_i = spikes_train[:, i, unit_i]\n",
    "            Y_test_i = spikes_test[:, i, unit_i]\n",
    "\n",
    "            X_train.append(X_train_i)\n",
    "            X_test.append(X_test_i)\n",
    "            Y_train.append(Y_train_i)\n",
    "            Y_test.append(Y_test_i)\n",
    "\n",
    "    X_train = np.array(X_train).reshape(-1, len(interesting_units)+8)\n",
    "    X_test = np.array(X_test).reshape(-1, len(interesting_units)+8)\n",
    "    Y_train = np.array(Y_train).reshape(-1, 1)\n",
    "    Y_test = np.array(Y_test).reshape(-1, 1)\n",
    "\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    Y_train_preds.append(reg.predict(X_train))\n",
    "    Y_test_preds.append(reg.predict(X_test))\n",
    "\n",
    "    X_train_dict[unit] = X_train\n",
    "    Y_train_dict[unit] = Y_train\n",
    "    X_test_dict[unit] = X_test\n",
    "    Y_test_dict[unit] = Y_test\n",
    "    reg_dict[unit] = reg\n",
    "    Y_train_preds_dict[unit] = Y_train_preds\n",
    "    Y_test_preds_dict[unit] = Y_test_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \"> \n",
    "    Examine the weights:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unit in interesting_units:\n",
    "    print(f'target_unit: {unit}')\n",
    "    for i, unit_coeff in enumerate(interesting_units):\n",
    "        print(f' unit {unit_coeff} coeff: {reg_dict[unit].coef_[0, i+8]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get functional connectivity matrix\n",
    "func_conn = []\n",
    "for unit in interesting_units:\n",
    "    print(f'target_unit: {unit}')\n",
    "    func_conn.append(reg_dict[unit].coef_[0, 8:])\n",
    "func_conn = np.array(func_conn)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6), dpi=150)\n",
    "im = ax.imshow(func_conn, cmap='RdBu', vmin=-1, vmax=1)\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(interesting_units)))\n",
    "ax.set_xticklabels(interesting_units, rotation=45)\n",
    "ax.set_yticks(np.arange(len(interesting_units)))\n",
    "ax.set_yticklabels(interesting_units)\n",
    "ax.set_xlabel('input unit')\n",
    "ax.set_ylabel('target unit')\n",
    "ax.set_title(\"Functional connectivity among VISp units\")\n",
    "fig.colorbar(im, ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Observation: </h3>\n",
    "    <ul>\n",
    "        <li> diagonal elements are strongly positive, indicating self recurrent effect. \n",
    "        <li> unit 951061715 might be an inhibitory unit as its effect on most other units are negative.\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> with other brain regions </h3>\n",
    "    <p> we can include units from other regions to examine potential interactions among different brain regions.\n",
    "Here let's use units from the primary visual cortex (VISp) and its upstream area, the lateral geniculate nuclues (LGd), as an example to examine how interaction among units from different brain regions might contibute to neural activity patterns.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_LGd = session.units[session.units[\"ecephys_structure_acronym\"] == 'LGd']\n",
    "unit_id_LGd = units_LGd.index.to_list()\n",
    "\n",
    "histograms_LGd = session.presentationwise_spike_counts(\n",
    "    stimulus_presentation_ids=presentations.index.values,  \n",
    "    bin_edges=time_bins,\n",
    "    unit_ids=unit_id_LGd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the list of interesting units predetermined by looking data/patrick_scratch/unit_frs_analysis figures\n",
    "spikes_train_with_LGd = np.concatenate([spikes.loc[train_idxs, :, interesting_units].values,\n",
    "                                        histograms_LGd.loc[train_idxs, :, unit_id_LGd].values], axis=2)\n",
    "\n",
    "spikes_test_with_LGd = np.concatenate([spikes.loc[test_idxs, :, interesting_units].values,\n",
    "                                       histograms_LGd.loc[test_idxs, :, unit_id_LGd].values], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_auto_reg_population(spikes, orientations, i):\n",
    "    \"\"\"\n",
    "    Generates a input data matrix for the 1 time bin auto regressive model\n",
    "    Args:\n",
    "        spikes: data array of num_trials x time steps x num_units\n",
    "        orientations: data array of num_trials\n",
    "        i: index of time bin\n",
    "    \"\"\"\n",
    "    orientations_one_hot = get_one_hot_encoding(orientations)\n",
    "    return np.concatenate([orientations_one_hot, spikes[:, i-1, :]], axis=1)\n",
    "\n",
    "\n",
    "X_train_dict, Y_train_dict = {}, {}\n",
    "X_test_dict, Y_test_dict = {}, {}\n",
    "reg_dict = {}\n",
    "Y_train_preds_dict = {}\n",
    "Y_test_preds_dict = {}\n",
    "\n",
    "total_unit = interesting_units+unit_id_LGd\n",
    "\n",
    "for unit_i, unit in enumerate(total_unit):\n",
    "    X_train, Y_train = [], []\n",
    "    X_test, Y_test = [], []\n",
    "\n",
    "    Y_train_preds = []\n",
    "    Y_test_preds = []\n",
    "\n",
    "    for i in range(spikes_train.shape[1]):\n",
    "        if i >= 1:\n",
    "            X_train_i = get_X_auto_reg_population(spikes_train_with_LGd, orientations_train, i)\n",
    "            X_test_i = get_X_auto_reg_population(spikes_test_with_LGd, orientations_test, i)\n",
    "            Y_train_i = spikes_train_with_LGd[:, i, unit_i]\n",
    "            Y_test_i = spikes_test_with_LGd[:, i, unit_i]\n",
    "\n",
    "            X_train.append(X_train_i)\n",
    "            X_test.append(X_test_i)\n",
    "            Y_train.append(Y_train_i)\n",
    "            Y_test.append(Y_test_i)\n",
    "\n",
    "    X_train = np.array(X_train).reshape(-1, len(total_unit)+8)\n",
    "    X_test = np.array(X_test).reshape(-1, len(total_unit)+8)\n",
    "    Y_train = np.array(Y_train).reshape(-1, 1)\n",
    "    Y_test = np.array(Y_test).reshape(-1, 1)\n",
    "\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_train, Y_train)\n",
    "\n",
    "    Y_train_preds.append(reg.predict(X_train))\n",
    "    Y_test_preds.append(reg.predict(X_test))\n",
    "\n",
    "    X_train_dict[unit] = X_train\n",
    "    Y_train_dict[unit] = Y_train\n",
    "    X_test_dict[unit] = X_test\n",
    "    Y_test_dict[unit] = Y_test\n",
    "    reg_dict[unit] = reg\n",
    "    Y_train_preds_dict[unit] = Y_train_preds\n",
    "    Y_test_preds_dict[unit] = Y_test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get functional connectivity matrix\n",
    "func_conn = []\n",
    "for unit in total_unit:\n",
    "    print(f'target_unit: {unit}')\n",
    "    func_conn.append(reg_dict[unit].coef_[0, 8:])\n",
    "func_conn = np.array(func_conn)\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6), dpi=150)\n",
    "im = ax.imshow(func_conn, cmap='RdBu', vmin=-1, vmax=1)\n",
    "ax.add_patch(Rectangle((-0.5, -0.5), len(interesting_units), len(interesting_units), \n",
    "             edgecolor='black', facecolor='red', fill=False, lw=1))\n",
    "ax.add_patch(Rectangle((-0.5+len(interesting_units), -0.5+len(interesting_units)), \n",
    "                    len(unit_id_LGd), len(unit_id_LGd), \n",
    "             edgecolor='black', facecolor='red', fill=False, lw=1))\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(total_unit)))\n",
    "ax.set_xticklabels(total_unit, rotation=75)\n",
    "ax.set_yticks(np.arange(len(total_unit)))\n",
    "ax.set_yticklabels(total_unit)\n",
    "ax.set_xlabel('input unit')\n",
    "ax.set_ylabel('target unit')\n",
    "ax.set_title(\"Functional connectivity among VISp & LGd units\")\n",
    "fig.colorbar(im, ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    <h3> Observation </h3>\n",
    "    <ul>\n",
    "        <li> Again, digonal elements indicate strong autocorrelation of neural activity\n",
    "        <li> in general the weights from VISp to LGd is more negative (the lower left quadrant), perhaps indicating the top-down feedback inhibition from VISp to LGd?\n",
    "    </ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
